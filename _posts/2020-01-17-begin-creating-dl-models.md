---
keywords: fastai
description: Steps to take to begin experimenting with deep learning.
title: Begin creating Deep LearningÂ models.
toc: false 
badges: true
comments: true
categories: [deep learning, ai]
nb_path: _notebooks/2020-01-17-begin-creating-dl-models.ipynb
layout: notebook
---

<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-01-17-begin-creating-dl-models.ipynb
-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Following the advice of <a href="https://jvns.ca/blog/2016/05/22/how-do-you-write-blog-posts/">Julia Evans</a>, I'm going to write about things that I wish I knew a year ago.
Machine Learning research sometimes feels like an activity reserved for the intellectually superior, while us mere mortals enjoy their trimmings when they publish and the open-source community implements it.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://www.scottcondron.com/images/copied_from_nb/my_icons/oliver.jpeg" alt="" title="Please, can I have an open-source implementation?"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="You-don't-need-a&#160;PhD">You don't need a&#160;PhD<a class="anchor-link" href="#You-don't-need-a&#160;PhD"> </a></h2><p>This myth was completely debunked for me when I took the fast.ai course, <a href="https://course.fast.ai/">Practical Deep Learning for Coders, v3</a>. So my first tip, do it. Also, <a href="https://www.fast.ai/2018/08/27/grad-school/">this post by Rachel Thomas</a> gives some great advice to those thinking about grad school.</p>
<p>If some of the stuff below seems like gibberish, fast.ai is a good place to go.</p>
<h2 id="Tips-for-people-wanting-to-begin-creating-/-improving-deep-learning&#160;models.">Tips for people wanting to begin creating / improving deep learning&#160;models.<a class="anchor-link" href="#Tips-for-people-wanting-to-begin-creating-/-improving-deep-learning&#160;models."> </a></h2><p>Find a dataset that interests you.
<a href="https://towardsdatascience.com/top-sources-for-machine-learning-datasets-bb6d0dc3378b">Here</a>'s a good post about where to find it. 
Try not to get analysis paralysis, just choose one. You're not married to the dataset you choose, but get to know it.</p>
<p><a href="https://www.kaggle.com/learn/overview">Here</a> are plenty of resources to learn to visualize and analyze a dataset.</p>
<h2 id="Use-Google&#160;Colab">Use Google&#160;Colab<a class="anchor-link" href="#Use-Google&#160;Colab"> </a></h2><p>If you want free access to a GPU, <a href="http://colab.research.google.com/">here you go</a>.
<a href="https://towardsdatascience.com/getting-started-with-google-colab-f2fff97f594c">If you want to know why or setting up a GPU is becoming frustrating.</a></p>
<h2 id="Get-100%-accuracy-on-one-instance/batch-of-your&#160;data">Get 100% accuracy on one instance/batch of your&#160;data<a class="anchor-link" href="#Get-100%-accuracy-on-one-instance/batch-of-your&#160;data"> </a></h2><p>Rather than wasting your time loading all of the data each time you want to check for bugs, create a <em>tiny dataset</em> with just one instance of your dataset and overfit it with a very simple model.</p>
<h2 id="&#160;Get-100%-accuracy-on-around-10%-of-your&#160;data">&#160;Get 100% accuracy on around 10% of your&#160;data<a class="anchor-link" href="#&#160;Get-100%-accuracy-on-around-10%-of-your&#160;data"> </a></h2><p>Once you can overfit your model on one instance, begin using more of your data and try to overfit it by adding more layers. Don't use any regularization for the moment (e.g., Dropout, L1/ L2 regularization), this is another sanity preserving tip so you know that your model is learning.</p>
<h2 id="&#160;Add-all-of-your-training&#160;data">&#160;Add all of your training&#160;data<a class="anchor-link" href="#&#160;Add-all-of-your-training&#160;data"> </a></h2><p>Once you add all of your data, if training is taking too long, leave it aside as an experiment and continue your work on the 10% of data. If you're overfitting when you add more layers and all of your data, here's five steps to reduce overfitting.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://www.scottcondron.com/images/copied_from_nb/my_icons/avoid_overfitting.jpeg" alt="" title="Source: fast.ai"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And that's it for today, those tips are some of the valuable gems that I wish I found out sooner.
If have any feedback, <a href="https://twitter.com/_ScottCondron">here's me on Twitter</a>.</p>
<p><em>Thanks for reading</em></p>

</div>
</div>
</div>
</div>
 

